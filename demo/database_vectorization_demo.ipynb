{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873adc83",
   "metadata": {},
   "source": [
    "# Database Vectorization Demo\n",
    "\n",
    "This notebook demonstrates how to use the Cocoon Database Vectorization module to:\n",
    "\n",
    "1. Process CSV files with product data\n",
    "2. Generate vector embeddings using AWS Bedrock\n",
    "3. Store results in S3 or local storage\n",
    "4. Track progress and handle errors\n",
    "\n",
    "This is a practical, production-ready example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a964cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /Users/sagarthacker/Tracera/repos/ml/cocoon to Python path\n",
      "‚úÖ Successfully imported DatabaseVectorizerService\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add parent directory to Python path so we can import cocoon\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "print(f\"Added {parent_dir} to Python path\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import the new DatabaseVectorizerService\n",
    "from cocoon.services.database_vectorizer import DatabaseVectorizerService\n",
    "\n",
    "print(\"‚úÖ Successfully imported DatabaseVectorizerService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c460615",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc49155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory: /Users/sagarthacker/Tracera/repos/ml/cocoon/demo/output\n",
      "Created temp directory: /Users/sagarthacker/Tracera/repos/ml/cocoon/demo/temp\n"
     ]
    }
   ],
   "source": [
    "# Create absolute paths for output directories\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "output_dir = os.path.join(current_dir, \"output\")\n",
    "temp_dir = os.path.join(current_dir, \"temp\")\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Created output directory: {output_dir}\")\n",
    "print(f\"Created temp directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7490f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping of database names to text columns\n",
    "database_to_text_column = {\n",
    "    \"naics\": \"2017 NAICS Title\",\n",
    "    \"defra\": \"EF Name\",\n",
    "    \"ecoinvent\": \"activity_name\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2013d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_pretty(config):\n",
    "    \"\"\"Print a VectorizationPipelineConfig in a user-friendly way.\"\"\"\n",
    "    def print_section(title, section_dict, indent=2):\n",
    "        print(f\"{title}:\")\n",
    "        for k, v in section_dict.items():\n",
    "            if isinstance(v, dict):\n",
    "                print(\" \" * indent + f\"{k}:\")\n",
    "                for subk, subv in v.items():\n",
    "                    print(\" \" * (indent + 2) + f\"{subk}: {subv}\")\n",
    "            else:\n",
    "                print(\" \" * indent + f\"{k}: {v}\")\n",
    "\n",
    "    print(\"Vectorization Pipeline Configuration\")\n",
    "    print(\"=\" * 35)\n",
    "    # Input\n",
    "    print_section(\"Input\", config.input.model_dump())\n",
    "    print()\n",
    "    # Processing\n",
    "    print_section(\"Processing\", config.processing.model_dump())\n",
    "    print()\n",
    "    # Vectorization\n",
    "    print_section(\"Vectorization\", config.vectorization.model_dump())\n",
    "    print()\n",
    "    # Output\n",
    "    print_section(\"Output\", config.output.model_dump())\n",
    "    print(\"=\" * 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef706f",
   "metadata": {},
   "source": [
    "## 3. NAICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66a3aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "database = \"naics\"\n",
    "\n",
    "# S3 vectorizer config\n",
    "naics_input_file_path=f\"s3://{os.getenv('READ_FILE_S3_BUCKET')}/{os.getenv('NAICS_DB_S3_PREFIX')}\"\n",
    "target_column=database_to_text_column[database]\n",
    "naics_output_path_prefix=f\"s3://{os.getenv('SAVE_RESULT_S3_BUCKET')}/{os.getenv('OUTPUT_PREFIX').format(database=database)}/{os.getenv('OUTPUT_FILENAME')}\"\n",
    "\n",
    "metadata_columns=[]\n",
    "output_format=\"csv\"\n",
    "\n",
    "# Additional processing options\n",
    "deduplicate_text=True\n",
    "text_cleaning=True\n",
    "min_text_length=1\n",
    "batch_size=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a285a09",
   "metadata": {},
   "source": [
    "### Titan-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f39579a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name = \"titan_v2\"\n",
    "os.environ[\"EMBEDDING_MODEL_ID\"] = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "naics_output_path = naics_output_path_prefix.format(embed_model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "670bc143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/raw/csv/2025/08/13/SupplyChainGHGEmissionFactors_v1.3.0_NAICS_CO2e_USD2022.csv\n",
      "Output file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/naics/titan_v2_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input file path: {naics_input_file_path}\")\n",
    "print(f\"Output file path: {naics_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1043274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Service initialized with default configuration\n",
      "Default region: eu-central-1\n",
      "Default model: amazon.titan-embed-text-v2:0\n",
      "Default batch size: 100\n",
      "\n",
      "üè• Service health check: healthy\n",
      "   ‚úÖ Embeddings available: True\n",
      "   ‚úÖ Test embedding dimensions: 1024\n"
     ]
    }
   ],
   "source": [
    "service = DatabaseVectorizerService()\n",
    "\n",
    "print(\"‚úÖ Service initialized with default configuration\")\n",
    "print(f\"Default region: {service.config.vectorization.embedding_model_config['aws_region_name']}\")\n",
    "print(f\"Default model: {service.config.vectorization.embedding_model_config['model_id']}\")\n",
    "print(f\"Default batch size: {service.config.vectorization.batch_size}\")\n",
    "\n",
    "# Check service health\n",
    "health = service.health_check()\n",
    "print(f\"\\nüè• Service health check: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"   ‚úÖ Embeddings available: {health['embeddings_available']}\")\n",
    "    print(f\"   ‚úÖ Test embedding dimensions: {health['test_embedding_dimensions']}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: {health.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f15791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization Pipeline Configuration\n",
      "===================================\n",
      "Input:\n",
      "  file_path: placeholder.csv\n",
      "  file_type: csv\n",
      "  csv_delimiter: ,\n",
      "  csv_encoding: utf-8\n",
      "  excel_sheet_name: None\n",
      "  excel_engine: openpyxl\n",
      "\n",
      "Processing:\n",
      "  deduplicate_text: True\n",
      "  preserve_original_indices: True\n",
      "  filter_empty_rows: True\n",
      "  text_cleaning: True\n",
      "  text_cleaning_options:\n",
      "    lowercase: True\n",
      "    remove_punctuation: False\n",
      "    remove_numbers: False\n",
      "    remove_special_chars: False\n",
      "    normalize_whitespace: True\n",
      "\n",
      "Vectorization:\n",
      "  embedding_model_config:\n",
      "    model_id: amazon.titan-embed-text-v2:0\n",
      "    aws_region_name: eu-central-1\n",
      "  target_column: text\n",
      "  metadata_columns: []\n",
      "  batch_size: 100\n",
      "\n",
      "Output:\n",
      "  output_path: output.parquet\n",
      "  output_format: parquet\n",
      "  compression: snappy\n",
      "  include_metadata: True\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "print_config_pretty(service.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "debf53c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing sample CSV with DatabaseVectorizerService...\n",
      "‚úÖ Processing completed in 185.50 seconds!\n",
      "   üìä Status: success\n",
      "   üìà Rows processed: 1016\n",
      "   üéØ Unique texts: 1016\n",
      "   üìÅ Output saved to: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/naics/titan_v2_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Process our sample CSV file with local storage\n",
    "print(\"üöÄ Processing sample CSV with DatabaseVectorizerService...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Process the file - this creates an immutable config for this operation\n",
    "    result = service.process_file(\n",
    "        input_file_path=naics_input_file_path,\n",
    "        target_column=target_column,\n",
    "        output_path=naics_output_path,\n",
    "        metadata_columns=metadata_columns,\n",
    "        output_format=output_format,\n",
    "        # Additional processing options\n",
    "        deduplicate_text=deduplicate_text,\n",
    "        text_cleaning=text_cleaning,\n",
    "        min_text_length=min_text_length,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Processing completed in {processing_time:.2f} seconds!\")\n",
    "    print(f\"   üìä Status: {result['status']}\")\n",
    "    print(f\"   üìà Rows processed: {result.get('rows_processed', 'N/A')}\")\n",
    "    print(f\"   üéØ Unique texts: {result.get('unique_texts', 'N/A')}\")\n",
    "    print(f\"   üìÅ Output saved to: {result.get('output_file', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b102ead",
   "metadata": {},
   "source": [
    "### Titan-G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed1b3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name = \"titan_g1\"\n",
    "os.environ[\"EMBEDDING_MODEL_ID\"] = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "naics_output_path = naics_output_path_prefix.format(embed_model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33e5c423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/raw/csv/2025/08/13/SupplyChainGHGEmissionFactors_v1.3.0_NAICS_CO2e_USD2022.csv\n",
      "Output file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/naics/titan_g1_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input file path: {naics_input_file_path}\")\n",
    "print(f\"Output file path: {naics_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1510af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Service initialized with default configuration\n",
      "Default region: eu-central-1\n",
      "Default model: amazon.titan-embed-text-v1\n",
      "Default batch size: 100\n",
      "\n",
      "üè• Service health check: healthy\n",
      "   ‚úÖ Embeddings available: True\n",
      "   ‚úÖ Test embedding dimensions: 1536\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Initialize with default configuration\n",
    "# This will use environment variables for AWS region, embedding model, etc.\n",
    "service = DatabaseVectorizerService()\n",
    "\n",
    "print(\"‚úÖ Service initialized with default configuration\")\n",
    "print(f\"Default region: {service.config.vectorization.embedding_model_config['aws_region_name']}\")\n",
    "print(f\"Default model: {service.config.vectorization.embedding_model_config['model_id']}\")\n",
    "print(f\"Default batch size: {service.config.vectorization.batch_size}\")\n",
    "\n",
    "# Check service health\n",
    "health = service.health_check()\n",
    "print(f\"\\nüè• Service health check: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"   ‚úÖ Embeddings available: {health['embeddings_available']}\")\n",
    "    print(f\"   ‚úÖ Test embedding dimensions: {health['test_embedding_dimensions']}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: {health.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76b09ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization Pipeline Configuration\n",
      "===================================\n",
      "Input:\n",
      "  file_path: placeholder.csv\n",
      "  file_type: csv\n",
      "  csv_delimiter: ,\n",
      "  csv_encoding: utf-8\n",
      "  excel_sheet_name: None\n",
      "  excel_engine: openpyxl\n",
      "\n",
      "Processing:\n",
      "  deduplicate_text: True\n",
      "  preserve_original_indices: True\n",
      "  filter_empty_rows: True\n",
      "  text_cleaning: True\n",
      "  text_cleaning_options:\n",
      "    lowercase: True\n",
      "    remove_punctuation: False\n",
      "    remove_numbers: False\n",
      "    remove_special_chars: False\n",
      "    normalize_whitespace: True\n",
      "\n",
      "Vectorization:\n",
      "  embedding_model_config:\n",
      "    model_id: amazon.titan-embed-text-v1\n",
      "    aws_region_name: eu-central-1\n",
      "  target_column: text\n",
      "  metadata_columns: []\n",
      "  batch_size: 100\n",
      "\n",
      "Output:\n",
      "  output_path: output.parquet\n",
      "  output_format: parquet\n",
      "  compression: snappy\n",
      "  include_metadata: True\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "print_config_pretty(service.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e086d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing sample CSV with DatabaseVectorizerService...\n",
      "‚úÖ Processing completed in 182.22 seconds!\n",
      "   üìä Status: success\n",
      "   üìà Rows processed: 1016\n",
      "   üéØ Unique texts: 1016\n",
      "   üìÅ Output saved to: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/naics/titan_g1_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Process our sample CSV file with local storage\n",
    "print(\"üöÄ Processing sample CSV with DatabaseVectorizerService...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Process the file - this creates an immutable config for this operation\n",
    "    result = service.process_file(\n",
    "        input_file_path=naics_input_file_path,\n",
    "        target_column=target_column,\n",
    "        output_path=naics_output_path,\n",
    "        metadata_columns=metadata_columns,\n",
    "        output_format=output_format,\n",
    "        # Additional processing options\n",
    "        deduplicate_text=deduplicate_text,\n",
    "        text_cleaning=text_cleaning,\n",
    "        min_text_length=min_text_length,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Processing completed in {processing_time:.2f} seconds!\")\n",
    "    print(f\"   üìä Status: {result['status']}\")\n",
    "    print(f\"   üìà Rows processed: {result.get('rows_processed', 'N/A')}\")\n",
    "    print(f\"   üéØ Unique texts: {result.get('unique_texts', 'N/A')}\")\n",
    "    print(f\"   üìÅ Output saved to: {result.get('output_file', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e814951",
   "metadata": {},
   "source": [
    "## 4. DEFRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a157b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "database = \"defra\"\n",
    "\n",
    "# S3 vectorizer config\n",
    "defra_input_file_path=f\"s3://{os.getenv('READ_FILE_S3_BUCKET')}/{os.getenv('DEFRA_DB_S3_PREFIX')}\"\n",
    "target_column=database_to_text_column[database]\n",
    "defra_output_path_prefix=f\"s3://{os.getenv('SAVE_RESULT_S3_BUCKET')}/{os.getenv('OUTPUT_PREFIX').format(database=database)}/{os.getenv('OUTPUT_FILENAME')}\"\n",
    "\n",
    "metadata_columns=[]\n",
    "output_format=\"csv\"\n",
    "\n",
    "# Additional processing options\n",
    "deduplicate_text=True\n",
    "text_cleaning=True\n",
    "min_text_length=1\n",
    "batch_size=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69157a9",
   "metadata": {},
   "source": [
    "### Titan-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854fb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name = \"titan_v2\"\n",
    "os.environ[\"EMBEDDING_MODEL_ID\"] = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "defra_output_path = defra_output_path_prefix.format(embed_model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a67fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/raw/csv/2025/08/13/20250308_DEFRA_S3_Emission_Factors_Database(Cat_1_&_2).csv\n",
      "Output file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/defra/titan_v2_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input file path: {defra_input_file_path}\")\n",
    "print(f\"Output file path: {defra_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca1f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Service initialized with default configuration\n",
      "Default region: eu-central-1\n",
      "Default model: amazon.titan-embed-text-v2:0\n",
      "Default batch size: 100\n",
      "\n",
      "üè• Service health check: healthy\n",
      "   ‚úÖ Embeddings available: True\n",
      "   ‚úÖ Test embedding dimensions: 1024\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Initialize with default configuration\n",
    "# This will use environment variables for AWS region, embedding model, etc.\n",
    "service = DatabaseVectorizerService()\n",
    "\n",
    "print(\"‚úÖ Service initialized with default configuration\")\n",
    "print(f\"Default region: {service.config.vectorization.embedding_model_config['aws_region_name']}\")\n",
    "print(f\"Default model: {service.config.vectorization.embedding_model_config['model_id']}\")\n",
    "print(f\"Default batch size: {service.config.vectorization.batch_size}\")\n",
    "\n",
    "# Check service health\n",
    "health = service.health_check()\n",
    "print(f\"\\nüè• Service health check: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"   ‚úÖ Embeddings available: {health['embeddings_available']}\")\n",
    "    print(f\"   ‚úÖ Test embedding dimensions: {health['test_embedding_dimensions']}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: {health.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc52fd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing sample CSV with DatabaseVectorizerService...\n",
      "‚úÖ Processing completed in 26.89 seconds!\n",
      "   üìä Status: success\n",
      "   üìà Rows processed: 134\n",
      "   üéØ Unique texts: 134\n",
      "   üìÅ Output saved to: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/defra/titan_v2_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Process our sample CSV file with local storage\n",
    "print(\"üöÄ Processing sample CSV with DatabaseVectorizerService...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Process the file - this creates an immutable config for this operation\n",
    "    result = service.process_file(\n",
    "        input_file_path=defra_input_file_path,\n",
    "        target_column=target_column,\n",
    "        output_path=defra_output_path,\n",
    "        metadata_columns=metadata_columns,\n",
    "        output_format=output_format,\n",
    "        # Additional processing options\n",
    "        deduplicate_text=deduplicate_text,\n",
    "        text_cleaning=text_cleaning,\n",
    "        min_text_length=min_text_length,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Processing completed in {processing_time:.2f} seconds!\")\n",
    "    print(f\"   üìä Status: {result['status']}\")\n",
    "    print(f\"   üìà Rows processed: {result.get('rows_processed', 'N/A')}\")\n",
    "    print(f\"   üéØ Unique texts: {result.get('unique_texts', 'N/A')}\")\n",
    "    print(f\"   üìÅ Output saved to: {result.get('output_file', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07c1bb",
   "metadata": {},
   "source": [
    "### Titan-G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39784474",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name = \"titan_g1\"\n",
    "os.environ[\"EMBEDDING_MODEL_ID\"] = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "defra_output_path = defra_output_path_prefix.format(embed_model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e20fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/raw/csv/2025/08/13/20250308_DEFRA_S3_Emission_Factors_Database(Cat_1_&_2).csv\n",
      "Output file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/defra/titan_g1_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input file path: {defra_input_file_path}\")\n",
    "print(f\"Output file path: {defra_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa8d4e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Service initialized with default configuration\n",
      "Default region: eu-central-1\n",
      "Default model: amazon.titan-embed-text-v1\n",
      "Default batch size: 100\n",
      "\n",
      "üè• Service health check: healthy\n",
      "   ‚úÖ Embeddings available: True\n",
      "   ‚úÖ Test embedding dimensions: 1536\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Initialize with default configuration\n",
    "# This will use environment variables for AWS region, embedding model, etc.\n",
    "service = DatabaseVectorizerService()\n",
    "\n",
    "print(\"‚úÖ Service initialized with default configuration\")\n",
    "print(f\"Default region: {service.config.vectorization.embedding_model_config['aws_region_name']}\")\n",
    "print(f\"Default model: {service.config.vectorization.embedding_model_config['model_id']}\")\n",
    "print(f\"Default batch size: {service.config.vectorization.batch_size}\")\n",
    "\n",
    "# Check service health\n",
    "health = service.health_check()\n",
    "print(f\"\\nüè• Service health check: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"   ‚úÖ Embeddings available: {health['embeddings_available']}\")\n",
    "    print(f\"   ‚úÖ Test embedding dimensions: {health['test_embedding_dimensions']}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: {health.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72a826e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing sample CSV with DatabaseVectorizerService...\n",
      "‚úÖ Processing completed in 27.21 seconds!\n",
      "   üìä Status: success\n",
      "   üìà Rows processed: 134\n",
      "   üéØ Unique texts: 134\n",
      "   üìÅ Output saved to: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/defra/titan_g1_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Process our sample CSV file with local storage\n",
    "print(\"üöÄ Processing sample CSV with DatabaseVectorizerService...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Process the file - this creates an immutable config for this operation\n",
    "    result = service.process_file(\n",
    "        input_file_path=defra_input_file_path,\n",
    "        target_column=target_column,\n",
    "        output_path=defra_output_path,\n",
    "        metadata_columns=metadata_columns,\n",
    "        output_format=output_format,\n",
    "        # Additional processing options\n",
    "        deduplicate_text=deduplicate_text,\n",
    "        text_cleaning=text_cleaning,\n",
    "        min_text_length=min_text_length,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Processing completed in {processing_time:.2f} seconds!\")\n",
    "    print(f\"   üìä Status: {result['status']}\")\n",
    "    print(f\"   üìà Rows processed: {result.get('rows_processed', 'N/A')}\")\n",
    "    print(f\"   üéØ Unique texts: {result.get('unique_texts', 'N/A')}\")\n",
    "    print(f\"   üìÅ Output saved to: {result.get('output_file', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d25aad",
   "metadata": {},
   "source": [
    "## 5. Ecoinvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "170a64aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "database = \"ecoinvent\"\n",
    "\n",
    "# S3 vectorizer config\n",
    "ecoinvent_input_file_path=f\"s3://{os.getenv('READ_FILE_S3_BUCKET')}/{os.getenv('ECOINVENT_DB_S3_PREFIX')}\"\n",
    "target_column=database_to_text_column[database]\n",
    "ecoinvent_output_path_prefix=f\"s3://{os.getenv('SAVE_RESULT_S3_BUCKET')}/{os.getenv('OUTPUT_PREFIX').format(database=database)}/{os.getenv('OUTPUT_FILENAME')}\"\n",
    "\n",
    "metadata_columns=[]\n",
    "output_format=\"csv\"\n",
    "\n",
    "# Additional processing options\n",
    "deduplicate_text=True\n",
    "text_cleaning=True\n",
    "min_text_length=1\n",
    "batch_size=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cfae0",
   "metadata": {},
   "source": [
    "### Titan-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75dc6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name = \"titan_v2\"\n",
    "os.environ[\"EMBEDDING_MODEL_ID\"] = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "ecoinvent_output_path = ecoinvent_output_path_prefix.format(embed_model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1512403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/raw/csv/2025/08/13/ecoinvent_impact_indicators.csv\n",
      "Output file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/ecoinvent/titan_v2_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input file path: {ecoinvent_input_file_path}\")\n",
    "print(f\"Output file path: {ecoinvent_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f1f8a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Service initialized with default configuration\n",
      "Default region: eu-central-1\n",
      "Default model: amazon.titan-embed-text-v2:0\n",
      "Default batch size: 100\n",
      "\n",
      "üè• Service health check: healthy\n",
      "   ‚úÖ Embeddings available: True\n",
      "   ‚úÖ Test embedding dimensions: 1024\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Initialize with default configuration\n",
    "# This will use environment variables for AWS region, embedding model, etc.\n",
    "service = DatabaseVectorizerService()\n",
    "\n",
    "print(\"‚úÖ Service initialized with default configuration\")\n",
    "print(f\"Default region: {service.config.vectorization.embedding_model_config['aws_region_name']}\")\n",
    "print(f\"Default model: {service.config.vectorization.embedding_model_config['model_id']}\")\n",
    "print(f\"Default batch size: {service.config.vectorization.batch_size}\")\n",
    "\n",
    "# Check service health\n",
    "health = service.health_check()\n",
    "print(f\"\\nüè• Service health check: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"   ‚úÖ Embeddings available: {health['embeddings_available']}\")\n",
    "    print(f\"   ‚úÖ Test embedding dimensions: {health['test_embedding_dimensions']}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: {health.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1c51847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing sample CSV with DatabaseVectorizerService...\n",
      "‚úÖ Processing completed in 1698.44 seconds!\n",
      "   üìä Status: success\n",
      "   üìà Rows processed: 9835\n",
      "   üéØ Unique texts: 9835\n",
      "   üìÅ Output saved to: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/ecoinvent/titan_v2_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Process our sample CSV file with local storage\n",
    "print(\"üöÄ Processing sample CSV with DatabaseVectorizerService...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Process the file - this creates an immutable config for this operation\n",
    "    result = service.process_file(\n",
    "        input_file_path=ecoinvent_input_file_path,\n",
    "        target_column=target_column,\n",
    "        output_path=ecoinvent_output_path,\n",
    "        metadata_columns=metadata_columns,\n",
    "        output_format=output_format,\n",
    "        # Additional processing options\n",
    "        deduplicate_text=deduplicate_text,\n",
    "        text_cleaning=text_cleaning,\n",
    "        min_text_length=min_text_length,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Processing completed in {processing_time:.2f} seconds!\")\n",
    "    print(f\"   üìä Status: {result['status']}\")\n",
    "    print(f\"   üìà Rows processed: {result.get('rows_processed', 'N/A')}\")\n",
    "    print(f\"   üéØ Unique texts: {result.get('unique_texts', 'N/A')}\")\n",
    "    print(f\"   üìÅ Output saved to: {result.get('output_file', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106322c",
   "metadata": {},
   "source": [
    "### Titan-G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71532d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name = \"titan_g1\"\n",
    "os.environ[\"EMBEDDING_MODEL_ID\"] = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "ecoinvent_output_path = ecoinvent_output_path_prefix.format(embed_model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "835dd2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/raw/csv/2025/08/13/ecoinvent_impact_indicators.csv\n",
      "Output file path: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/ecoinvent/titan_g1_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input file path: {ecoinvent_input_file_path}\")\n",
    "print(f\"Output file path: {ecoinvent_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d031e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Service initialized with default configuration\n",
      "Default region: eu-central-1\n",
      "Default model: amazon.titan-embed-text-v1\n",
      "Default batch size: 100\n",
      "\n",
      "üè• Service health check: healthy\n",
      "   ‚úÖ Embeddings available: True\n",
      "   ‚úÖ Test embedding dimensions: 1536\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Initialize with default configuration\n",
    "# This will use environment variables for AWS region, embedding model, etc.\n",
    "service = DatabaseVectorizerService()\n",
    "\n",
    "print(\"‚úÖ Service initialized with default configuration\")\n",
    "print(f\"Default region: {service.config.vectorization.embedding_model_config['aws_region_name']}\")\n",
    "print(f\"Default model: {service.config.vectorization.embedding_model_config['model_id']}\")\n",
    "print(f\"Default batch size: {service.config.vectorization.batch_size}\")\n",
    "\n",
    "# Check service health\n",
    "health = service.health_check()\n",
    "print(f\"\\nüè• Service health check: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"   ‚úÖ Embeddings available: {health['embeddings_available']}\")\n",
    "    print(f\"   ‚úÖ Test embedding dimensions: {health['test_embedding_dimensions']}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: {health.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13a7d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing sample CSV with DatabaseVectorizerService...\n",
      "‚úÖ Processing completed in 1712.76 seconds!\n",
      "   üìä Status: success\n",
      "   üìà Rows processed: 9835\n",
      "   üéØ Unique texts: 9835\n",
      "   üìÅ Output saved to: s3://tracera-ml-prod-data/projects/emission_factor_matching/org_data/common/features/v1.0.0/ecoinvent/titan_g1_embeddings_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Process our sample CSV file with local storage\n",
    "print(\"üöÄ Processing sample CSV with DatabaseVectorizerService...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Process the file - this creates an immutable config for this operation\n",
    "    result = service.process_file(\n",
    "        input_file_path=ecoinvent_input_file_path,\n",
    "        target_column=target_column,\n",
    "        output_path=ecoinvent_output_path,\n",
    "        metadata_columns=metadata_columns,\n",
    "        output_format=output_format,\n",
    "        # Additional processing options\n",
    "        deduplicate_text=deduplicate_text,\n",
    "        text_cleaning=text_cleaning,\n",
    "        min_text_length=min_text_length,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Processing completed in {processing_time:.2f} seconds!\")\n",
    "    print(f\"   üìä Status: {result['status']}\")\n",
    "    print(f\"   üìà Rows processed: {result.get('rows_processed', 'N/A')}\")\n",
    "    print(f\"   üéØ Unique texts: {result.get('unique_texts', 'N/A')}\")\n",
    "    print(f\"   üìÅ Output saved to: {result.get('output_file', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e464e9",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaba040",
   "metadata": {},
   "source": [
    "## Sample CSV Data - Demo\n",
    "\n",
    "First, let's create a sample CSV file with product data to demonstrate the vectorization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 20 duplicate product titles with different attributes\n",
      "Created sample CSV with 120 products at sample_products.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Premium Cotton T-Shirt - Size M</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>10.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Stainless Steel Cookware Set - 3 Piece</td>\n",
       "      <td>Home &amp; Kitchen</td>\n",
       "      <td>11.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Product 3 - Sports &amp; Outdoors Item</td>\n",
       "      <td>Sports &amp; Outdoors</td>\n",
       "      <td>12.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Product 4 - Beauty &amp; Personal Care Item</td>\n",
       "      <td>Beauty &amp; Personal Care</td>\n",
       "      <td>13.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Product 5 - Toys &amp; Games Item</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>14.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                            product_title  \\\n",
       "0           1          Premium Cotton T-Shirt - Size M   \n",
       "1           2   Stainless Steel Cookware Set - 3 Piece   \n",
       "2           3       Product 3 - Sports & Outdoors Item   \n",
       "3           4  Product 4 - Beauty & Personal Care Item   \n",
       "4           5            Product 5 - Toys & Games Item   \n",
       "\n",
       "                 category  price  \n",
       "0                Clothing  10.99  \n",
       "1          Home & Kitchen  11.99  \n",
       "2       Sports & Outdoors  12.99  \n",
       "3  Beauty & Personal Care  13.99  \n",
       "4            Toys & Games  14.99  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_sample_csv(num_products=100, output_path=\"sample_products.csv\", add_duplicates=True):\n",
    "    \"\"\"Create a sample CSV file with product data.\"\"\"\n",
    "    \n",
    "    # Create sample product data\n",
    "    products = []\n",
    "    \n",
    "    # Define some categories\n",
    "    categories = [\n",
    "        \"Electronics\", \"Clothing\", \"Home & Kitchen\", \"Sports & Outdoors\",\n",
    "        \"Beauty & Personal Care\", \"Toys & Games\", \"Books\", \"Office Products\"\n",
    "    ]\n",
    "    \n",
    "    # Generate product data\n",
    "    for i in range(1, num_products + 1):\n",
    "        category = categories[i % len(categories)]\n",
    "        product_title = f\"Product {i} - {category} Item\"\n",
    "        \n",
    "        # Add more descriptive titles for specific categories\n",
    "        if category == \"Electronics\":\n",
    "            product_title = f\"Wireless Bluetooth Headphones - Model X{i}\"\n",
    "        elif category == \"Clothing\":\n",
    "            product_title = f\"Premium Cotton T-Shirt - Size {['S', 'M', 'L', 'XL'][i % 4]}\"\n",
    "        elif category == \"Home & Kitchen\":\n",
    "            product_title = f\"Stainless Steel Cookware Set - {i % 10 + 1} Piece\"\n",
    "        \n",
    "        products.append({\n",
    "            \"product_id\": i,\n",
    "            \"product_title\": product_title,\n",
    "            \"category\": category,\n",
    "            \"price\": round((i % 50) + 9.99, 2)\n",
    "        })\n",
    "    \n",
    "    # Add duplicate products with different product IDs to demonstrate grouping\n",
    "    if add_duplicates:\n",
    "        # Add duplicates for 20% of products\n",
    "        num_duplicates = num_products // 5\n",
    "        for i in range(num_duplicates):\n",
    "            # Pick a random product to duplicate\n",
    "            original_idx = i * 5  # Evenly distribute duplicates\n",
    "            duplicate = products[original_idx].copy()\n",
    "            duplicate[\"product_id\"] = num_products + i + 1\n",
    "            # Keep the same product title but vary other attributes\n",
    "            duplicate[\"price\"] = round(duplicate[\"price\"] + 5.0, 2)  # Different price\n",
    "            products.append(duplicate)\n",
    "        \n",
    "        print(f\"Added {num_duplicates} duplicate product titles with different attributes\")\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Created sample CSV with {len(products)} products at {output_path}\")\n",
    "    return df\n",
    "\n",
    "# Create a sample CSV with 100 products plus duplicates\n",
    "sample_df = create_sample_csv(num_products=100)\n",
    "sample_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82cdb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Premium Cotton T-Shirt - Size M</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>18.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Premium Cotton T-Shirt - Size M</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>26.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Premium Cotton T-Shirt - Size M</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>34.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Premium Cotton T-Shirt - Size M</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>42.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>Premium Cotton T-Shirt - Size M</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>50.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_id                    product_title  category  price\n",
       "8            9  Premium Cotton T-Shirt - Size M  Clothing  18.99\n",
       "16          17  Premium Cotton T-Shirt - Size M  Clothing  26.99\n",
       "24          25  Premium Cotton T-Shirt - Size M  Clothing  34.99\n",
       "32          33  Premium Cotton T-Shirt - Size M  Clothing  42.99\n",
       "40          41  Premium Cotton T-Shirt - Size M  Clothing  50.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Duplicates\n",
    "sample_df[sample_df.duplicated(subset=[\"product_title\"])].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4da4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unique product titles\n",
    "sample_df.product_title.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63839623",
   "metadata": {},
   "source": [
    "### Initialize the Service\n",
    "\n",
    "The service can be initialized with default configuration or with a custom configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Service initialized with default configuration\n",
      "Default region: eu-central-1\n",
      "Default model: amazon.titan-embed-text-v1\n",
      "Default batch size: 100\n",
      "\n",
      "üè• Service health check: healthy\n",
      "   ‚úÖ Embeddings available: True\n",
      "   ‚úÖ Test embedding dimensions: 1536\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Initialize with default configuration\n",
    "# This will use environment variables for AWS region, embedding model, etc.\n",
    "service = DatabaseVectorizerService()\n",
    "\n",
    "print(\"‚úÖ Service initialized with default configuration\")\n",
    "print(f\"Default region: {service.default_config.vectorization.embedding_model_config['aws_region_name']}\")\n",
    "print(f\"Default model: {service.default_config.vectorization.embedding_model_config['model_id']}\")\n",
    "print(f\"Default batch size: {service.default_config.vectorization.batch_size}\")\n",
    "\n",
    "# Check service health\n",
    "health = service.health_check()\n",
    "print(f\"\\nüè• Service health check: {health['status']}\")\n",
    "if health['status'] == 'healthy':\n",
    "    print(f\"   ‚úÖ Embeddings available: {health['embeddings_available']}\")\n",
    "    print(f\"   ‚úÖ Test embedding dimensions: {health['test_embedding_dimensions']}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Error: {health.get('error', 'Unknown error')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29284d74",
   "metadata": {},
   "source": [
    "### Process File with Local Storage\n",
    "\n",
    "The service provides a simple `process_file` method that handles the entire vectorization pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing sample CSV with DatabaseVectorizerService...\n",
      "‚úÖ Processing completed in 14.97 seconds!\n",
      "   üìä Status: success\n",
      "   üìà Rows processed: 80\n",
      "   üéØ Unique texts: 80\n",
      "   üìÅ Output saved to: /Users/sagarthacker/Tracera/repos/ml/cocoon/demo/output/service_demo_output.parquet\n",
      "\n",
      "üìã Output preview (80 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>embedding</th>\n",
       "      <th>index_ids</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Premium Cotton T-Shirt - Size M</td>\n",
       "      <td>[-0.1533203125, 0.439453125, 0.07421875, 0.679...</td>\n",
       "      <td>[0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88,...</td>\n",
       "      <td>1</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>10.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product 100 - Beauty &amp; Personal Care Item</td>\n",
       "      <td>[0.52734375, 0.5234375, 0.2734375, 0.51171875,...</td>\n",
       "      <td>[99]</td>\n",
       "      <td>100</td>\n",
       "      <td>Beauty &amp; Personal Care</td>\n",
       "      <td>9.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product 11 - Sports &amp; Outdoors Item</td>\n",
       "      <td>[0.61328125, 0.40625, -0.267578125, 0.11279296...</td>\n",
       "      <td>[10, 102]</td>\n",
       "      <td>11</td>\n",
       "      <td>Sports &amp; Outdoors</td>\n",
       "      <td>20.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product 12 - Beauty &amp; Personal Care Item</td>\n",
       "      <td>[0.5390625, 0.390625, 0.1416015625, 0.05346679...</td>\n",
       "      <td>[11]</td>\n",
       "      <td>12</td>\n",
       "      <td>Beauty &amp; Personal Care</td>\n",
       "      <td>21.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product 13 - Toys &amp; Games Item</td>\n",
       "      <td>[0.4609375, 0.138671875, -0.462890625, 0.09082...</td>\n",
       "      <td>[12]</td>\n",
       "      <td>13</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>22.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       label  \\\n",
       "0            Premium Cotton T-Shirt - Size M   \n",
       "1  Product 100 - Beauty & Personal Care Item   \n",
       "2        Product 11 - Sports & Outdoors Item   \n",
       "3   Product 12 - Beauty & Personal Care Item   \n",
       "4             Product 13 - Toys & Games Item   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [-0.1533203125, 0.439453125, 0.07421875, 0.679...   \n",
       "1  [0.52734375, 0.5234375, 0.2734375, 0.51171875,...   \n",
       "2  [0.61328125, 0.40625, -0.267578125, 0.11279296...   \n",
       "3  [0.5390625, 0.390625, 0.1416015625, 0.05346679...   \n",
       "4  [0.4609375, 0.138671875, -0.462890625, 0.09082...   \n",
       "\n",
       "                                           index_ids  product_id  \\\n",
       "0  [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88,...           1   \n",
       "1                                               [99]         100   \n",
       "2                                          [10, 102]          11   \n",
       "3                                               [11]          12   \n",
       "4                                               [12]          13   \n",
       "\n",
       "                 category  price  \n",
       "0                Clothing  10.99  \n",
       "1  Beauty & Personal Care   9.99  \n",
       "2       Sports & Outdoors  20.99  \n",
       "3  Beauty & Personal Care  21.99  \n",
       "4            Toys & Games  22.99  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Column information:\n",
      "   - label: object\n",
      "   - embedding: object\n",
      "   - index_ids: object\n",
      "   - product_id: int64\n",
      "   - category: object\n",
      "   - price: float64\n"
     ]
    }
   ],
   "source": [
    "# Process our sample CSV file with local storage\n",
    "print(\"üöÄ Processing sample CSV with DatabaseVectorizerService...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Process the file - this creates an immutable config for this operation\n",
    "    result = service.process_file(\n",
    "        input_file_path=\"sample_products.csv\",\n",
    "        target_column=\"product_title\",\n",
    "        output_path=os.path.join(output_dir, \"service_demo_output.parquet\"),\n",
    "        metadata_columns=[\"product_id\", \"category\", \"price\"],\n",
    "        output_format=\"parquet\",\n",
    "        # Additional processing options\n",
    "        deduplicate_text=True,\n",
    "        text_cleaning=True,\n",
    "        min_text_length=5,\n",
    "        batch_size=50\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Processing completed in {processing_time:.2f} seconds!\")\n",
    "    print(f\"   üìä Status: {result['status']}\")\n",
    "    print(f\"   üìà Rows processed: {result.get('rows_processed', 'N/A')}\")\n",
    "    print(f\"   üéØ Unique texts: {result.get('unique_texts', 'N/A')}\")\n",
    "    print(f\"   üìÅ Output saved to: {result.get('output_file', 'N/A')}\")\n",
    "    \n",
    "    # Display the output\n",
    "    if result['status'] == 'success' and 'output_file' in result:\n",
    "        output_file = result['output_file']\n",
    "        if os.path.exists(output_file):\n",
    "            output_df = pd.read_parquet(output_file)\n",
    "            print(f\"\\nüìã Output preview ({len(output_df)} rows):\")\n",
    "            display(output_df.head())\n",
    "            \n",
    "            print(f\"\\nüîç Column information:\")\n",
    "            for col in output_df.columns:\n",
    "                print(f\"   - {col}: {output_df[col].dtype}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Output file not found at: {output_file}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3607e",
   "metadata": {},
   "source": [
    "### Process File with S3 Storage\n",
    "\n",
    "The same service can be used to process files and store results in S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Processing sample CSV with S3 storage...\n",
      "‚úÖ S3 processing completed in 16.62 seconds!\n",
      "   üìä Status: success\n",
      "   üìà Rows processed: 80\n",
      "   üéØ Unique texts: 80\n",
      "   üìÅ S3 output: s3://esgflo-ml-test-scope3/test/database_vectorization_demo/service_demo_output.csv\n",
      "\n",
      "üåü Successfully saved results to S3!\n",
      "   üìç S3 location: s3://esgflo-ml-test-scope3/test/database_vectorization_demo/service_demo_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Process with S3 storage\n",
    "print(\"üöÄ Processing sample CSV with S3 storage...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # S3 output path\n",
    "    s3_output_path = f\"s3://esgflo-ml-test-scope3/test/database_vectorization_demo/service_demo_output.csv\"\n",
    "    \n",
    "    # Process the file with S3 storage\n",
    "    result = service.process_file(\n",
    "        input_file_path=\"sample_products.csv\",\n",
    "        target_column=\"product_title\", \n",
    "        output_path=s3_output_path,\n",
    "        metadata_columns=[\"product_id\", \"category\", \"price\"],\n",
    "        output_format=\"csv\",  # CSV format for S3\n",
    "        # Processing options\n",
    "        deduplicate_text=True,\n",
    "        text_cleaning=True,\n",
    "        batch_size=25\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ S3 processing completed in {processing_time:.2f} seconds!\")\n",
    "    print(f\"   üìä Status: {result['status']}\")\n",
    "    print(f\"   üìà Rows processed: {result.get('rows_processed', 'N/A')}\")\n",
    "    print(f\"   üéØ Unique texts: {result.get('unique_texts', 'N/A')}\")\n",
    "    print(f\"   üìÅ S3 output: {result.get('output_file', 'N/A')}\")\n",
    "    \n",
    "    # The service automatically detects S3 paths and uses S3 storage\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"\\nüåü Successfully saved results to S3!\")\n",
    "        print(f\"   üìç S3 location: {result.get('output_file')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during S3 processing: {str(e)}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37874c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'error',\n",
       " 'error_message': \"Unrecognized compression type: snappy\\nValid compression types are ['infer', None, 'bz2', 'gzip', 'tar', 'xz', 'zip', 'zstd']\",\n",
       " 'error_type': 'ValueError',\n",
       " 'input_file': 'sample_products.csv',\n",
       " 'target_column': 'product_title',\n",
       " 'output_path': 's3://esgflo-ml-test-scope3/test/database_vectorization_demo/service_demo_output.csv'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868af79",
   "metadata": {},
   "source": [
    "### Initialize Service with Custom Configuration\n",
    "\n",
    "You can also initialize the service with a custom configuration for more control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom service initialized\n",
      "   üîß Min text length: 10\n",
      "   üîß Max text length: 200\n",
      "   üîß Batch size: 20\n",
      "   üîß Compression: gzip\n"
     ]
    }
   ],
   "source": [
    "# Create a custom configuration\n",
    "from cocoon.core.config.models import (\n",
    "    VectorizationPipelineConfig, FileInputConfig, ProcessingConfig,\n",
    "    VectorizationConfig, OutputConfig\n",
    ")\n",
    "\n",
    "# Create custom configuration with specific settings\n",
    "custom_config = VectorizationPipelineConfig(\n",
    "    input=FileInputConfig(\n",
    "        file_path=\"placeholder.csv\",\n",
    "        file_type=\"csv\",\n",
    "        csv_delimiter=\",\",\n",
    "        csv_encoding=\"utf-8\"\n",
    "    ),\n",
    "    processing=ProcessingConfig(\n",
    "        deduplicate_text=True,\n",
    "        preserve_original_indices=True,\n",
    "        text_cleaning=True,\n",
    "        min_text_length=10,  # Longer minimum text length\n",
    "        max_text_length=200  # Maximum text length\n",
    "    ),\n",
    "    vectorization=VectorizationConfig(\n",
    "        embedding_model_config={\n",
    "            \"model_id\": \"amazon.titan-embed-text-v1\",\n",
    "            \"aws_region_name\": \"eu-central-1\"\n",
    "        },\n",
    "        target_column=\"text\",\n",
    "        metadata_columns=[],\n",
    "        batch_size=20  # Smaller batch size\n",
    "    ),\n",
    "    output=OutputConfig(\n",
    "        output_path=\"output.parquet\",\n",
    "        output_format=\"parquet\",\n",
    "        compression=\"gzip\",  # Different compression\n",
    "        include_metadata=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initialize service with custom config\n",
    "custom_service = DatabaseVectorizerService(custom_config)\n",
    "\n",
    "print(\"‚úÖ Custom service initialized\")\n",
    "print(f\"   üîß Min text length: {custom_service.default_config.processing.min_text_length}\")\n",
    "print(f\"   üîß Max text length: {custom_service.default_config.processing.max_text_length}\")\n",
    "print(f\"   üîß Batch size: {custom_service.default_config.vectorization.batch_size}\")\n",
    "print(f\"   üîß Compression: {custom_service.default_config.output.compression}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b370615",
   "metadata": {},
   "source": [
    "### Error Handling and Statistics\n",
    "\n",
    "The service provides comprehensive error handling and statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing non_existent_file.csv: CSV file not found: non_existent_file.csv\n",
      "Error processing sample_products.csv: Target column 'non_existent_column' not found in input file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing error handling with non-existent file...\n",
      "üìã Error handling result:\n",
      "   üìä Status: error\n",
      "   ‚ùå Error type: FileNotFoundError\n",
      "   üìù Error message: CSV file not found: non_existent_file.csv\n",
      "   üìÅ Input file: non_existent_file.csv\n",
      "\n",
      "üß™ Testing error handling with invalid column name...\n",
      "üìã Invalid column result:\n",
      "   üìä Status: error\n",
      "   ‚ùå Error type: ValueError\n",
      "   üìù Error message: Target column 'non_existent_column' not found in input file...\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate error handling by trying to process a non-existent file\n",
    "print(\"üß™ Testing error handling with non-existent file...\")\n",
    "\n",
    "error_result = service.process_file(\n",
    "    input_file_path=\"non_existent_file.csv\",\n",
    "    target_column=\"text_column\",\n",
    "    output_path=\"error_output.parquet\"\n",
    ")\n",
    "\n",
    "print(f\"üìã Error handling result:\")\n",
    "print(f\"   üìä Status: {error_result['status']}\")\n",
    "print(f\"   ‚ùå Error type: {error_result.get('error_type', 'N/A')}\")\n",
    "print(f\"   üìù Error message: {error_result.get('error_message', 'N/A')}\")\n",
    "print(f\"   üìÅ Input file: {error_result.get('input_file', 'N/A')}\")\n",
    "\n",
    "# Demonstrate with invalid column name\n",
    "print(f\"\\nüß™ Testing error handling with invalid column name...\")\n",
    "\n",
    "invalid_column_result = service.process_file(\n",
    "    input_file_path=\"sample_products.csv\",\n",
    "    target_column=\"non_existent_column\",\n",
    "    output_path=\"invalid_output.parquet\"\n",
    ")\n",
    "\n",
    "print(f\"üìã Invalid column result:\")\n",
    "print(f\"   üìä Status: {invalid_column_result['status']}\")\n",
    "print(f\"   ‚ùå Error type: {invalid_column_result.get('error_type', 'N/A')}\")\n",
    "print(f\"   üìù Error message: {invalid_column_result.get('error_message', 'N/A')[:100]}...\")  # Truncate long messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823afed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dac916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocoon-6lRbKluw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
